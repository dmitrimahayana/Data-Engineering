{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8c498d-0599-4a6e-a311-fbaaf61227b3",
   "metadata": {},
   "source": [
    "# Use Case 1 - Calculate Area of Land Cove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d17a0-cebc-4e6b-8d25-d25e28c587d0",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48019c43-f3a9-4c06-9c3d-6e1ef8d8b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=vtAe2GWIhIhtcL5cillEpgGw9PCpY_9X-Oalog9c0G4&tc=pf8kEqZJLK5lN-4SKREW1-oyKnZyuzhMEz5DmVgXCZE&cc=jbo9BZh2g4x81aBL1i81Jb-7Btok6ywBZ7tCfrJ_8so>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=vtAe2GWIhIhtcL5cillEpgGw9PCpY_9X-Oalog9c0G4&tc=pf8kEqZJLK5lN-4SKREW1-oyKnZyuzhMEz5DmVgXCZE&cc=jbo9BZh2g4x81aBL1i81Jb-7Btok6ywBZ7tCfrJ_8so</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1Adeu5BU11h1fAQet9Q05Ww8Tc1uIr-rukexl-wUT2TACwMfRlg-huibIaH0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Google Earth Initialization\n",
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd7e936-c768-4645-86e1-eb84b5c87287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n"
     ]
    }
   ],
   "source": [
    "# Matplotlib Initialization\n",
    "from simpledbf import Dbf5\n",
    "from dbfread import DBF\n",
    "from geopy.geocoders import Nominatim\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, DoubleType\n",
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geemap\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1edfe-cfbb-43a3-9ab0-572cfc57e6e2",
   "metadata": {},
   "source": [
    "## Load Indonesia Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e498b62d-94bb-45dc-89de-11a5fcfed124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Spark Session\n",
    "sparkMaster = 'local[*]'\n",
    "sparkAppName = 'Py-RevalueNature-Case2'\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(sparkMaster) \\\n",
    "    .appName(sparkAppName) \\\n",
    "    .config('spark.jars.packages', 'com.github.sandrasi.geodata:geopy-spark_2.12:1.0.0') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b91a959-f5b3-43ec-8804-474312eb4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+----+---------+------+-----------+--------+-------+--------+--------+--------+--------+\n",
      "|   _c0|    _c1|  _c2| _c3|      _c4|   _c5|        _c6|     _c7|    _c8|     _c9|    _c10|    _c11|    _c12|\n",
      "+------+-------+-----+----+---------+------+-----------+--------+-------+--------+--------+--------+--------+\n",
      "|Number|Digimon|Stage|Type|Attribute|Memory|Equip Slots|Lv 50 HP|Lv50 SP|Lv50 Atk|Lv50 Def|Lv50 Int|Lv50 Spd|\n",
      "|     1|Kuramon| Baby|Free|  Neutral|     2|          0|     590|     77|      79|      69|      68|      95|\n",
      "|     2|Pabumon| Baby|Free|  Neutral|     2|          0|     950|     62|      76|      76|      69|      68|\n",
      "|     3|Punimon| Baby|Free|  Neutral|     2|          0|     870|     50|      97|      87|      50|      75|\n",
      "|     4|Botamon| Baby|Free|  Neutral|     2|          0|     690|     68|      77|      95|      76|      61|\n",
      "+------+-------+-----+----+---------+------+-----------+--------+-------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test Load Digimon Dataset using Spark\n",
    "digimon_folderpath = 'D:\\\\00 Project\\\\00 My Project\\\\Dataset\\\\Digimon Dataset\\\\'\n",
    "digimon_filename = 'DigiDB_digimonlist.csv'\n",
    "df_digimon = spark.read.format(\"csv\").load(digimon_folderpath+digimon_filename)\n",
    "print(df_digimon.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c67961d6-58f2-459f-8ee7-f460a9db4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+------+---------+----------+-----------+----------------+-------------+-----------+---------+------+---------+----------+\n",
      "|GID_0|   NAME_0|  GID_1|NAME_1|    GID_2|    NAME_2|      GID_3|          NAME_3|        GID_4|     NAME_4|VARNAME_4|TYPE_4|ENGTYPE_4|      CC_4|\n",
      "+-----+---------+-------+------+---------+----------+-----------+----------------+-------------+-----------+---------+------+---------+----------+\n",
      "|  IDN|Indonesia|IDN.1_1|  Aceh|IDN.1.2_1|Aceh Barat|IDN.1.2.1_1|Arongan Lambalek|IDN.1.2.1.1_1| Alue Bagok|         |  Desa|  Village|1107062026|\n",
      "|  IDN|Indonesia|IDN.1_1|  Aceh|IDN.1.2_1|Aceh Barat|IDN.1.2.1_1|Arongan Lambalek|IDN.1.2.1.2_1| Alue Batee|         |  Desa|  Village|1107062027|\n",
      "|  IDN|Indonesia|IDN.1_1|  Aceh|IDN.1.2_1|Aceh Barat|IDN.1.2.1_1|Arongan Lambalek|IDN.1.2.1.3_1|Alue Sundak|         |  Desa|  Village|1107062025|\n",
      "|  IDN|Indonesia|IDN.1_1|  Aceh|IDN.1.2_1|Aceh Barat|IDN.1.2.1_1|Arongan Lambalek|IDN.1.2.1.4_1|    Arongan|         |  Desa|  Village|1107062005|\n",
      "|  IDN|Indonesia|IDN.1_1|  Aceh|IDN.1.2_1|Aceh Barat|IDN.1.2.1_1|Arongan Lambalek|IDN.1.2.1.5_1|  Cot Buloh|         |  Desa|  Village|1107062014|\n",
      "+-----+---------+-------+------+---------+----------+-----------+----------------+-------------+-----------+---------+------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load Indonesia Dataset using Spark\n",
    "file_path = 'D:\\\\00 Project\\\\00 My Project\\\\Dataset\\\\Revalue Nature\\\\Case 1\\\\gadm36_IDN_shp\\\\'\n",
    "file_4 = 'gadm36_IDN_4.dbf'\n",
    "records = DBF(file_path+file_4, encoding=\"latin1\")\n",
    "\n",
    "# define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"GID_0\", StringType(), True),\n",
    "    StructField(\"NAME_0\", StringType(), True),\n",
    "    StructField(\"GID_1\", StringType(), True),\n",
    "    StructField(\"NAME_1\", StringType(), True),\n",
    "    StructField(\"GID_2\", StringType(), True),\n",
    "    StructField(\"NAME_2\", StringType(), True),\n",
    "    StructField(\"GID_3\", StringType(), True),\n",
    "    StructField(\"NAME_3\", StringType(), True),\n",
    "    StructField(\"GID_4\", StringType(), True),\n",
    "    StructField(\"NAME_4\", StringType(), True),\n",
    "    StructField(\"VARNAME_4\", StringType(), True),\n",
    "    StructField(\"TYPE_4\", StringType(), True),\n",
    "    StructField(\"ENGTYPE_4\", StringType(), True),\n",
    "    StructField(\"CC_4\", StringType(), True),\n",
    "    # add more fields as needed\n",
    "])\n",
    "\n",
    "# create a list of dictionaries containing the data\n",
    "data = [dict(record) for record in records]\n",
    "\n",
    "# create a PySpark DataFrame from the data\n",
    "df_indonesia = spark.createDataFrame(data, schema)\n",
    "print(df_indonesia.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a67fbcd-c053-4277-9068-8eb2d027b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longitude of the location is:  113.54024450668652\n",
      "The latitude of the location is:  -8.28236725\n"
     ]
    }
   ],
   "source": [
    "# Initialize Nominatim API\n",
    "geolocator = Nominatim(user_agent=\"Geo Locator\")\n",
    "\n",
    "def get_coordinate(location):\n",
    "    try:\n",
    "        coordinate = geolocator.geocode(location)\n",
    "        return (coordinate.longitude, coordinate.latitude)\n",
    "    except:\n",
    "        # return {'longitude': 0, 'latitude': 0}\n",
    "        return (None, None)\n",
    "\n",
    "addr = \"Indonesia,Jawa Timur,Jember,Balung\"\n",
    "coordinate = get_coordinate(addr)\n",
    "print(\"The longitude of the location is: \", coordinate[0])\n",
    "print(\"The latitude of the location is: \", coordinate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "957a5784-1ad3-4d5d-8548-2c18cb5d0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy into new Dataframe\n",
    "df_coordinate = df_indonesia\n",
    "\n",
    "# Get Long and Lat\n",
    "def get_long(value):\n",
    "    return get_coordinate(value)[0]\n",
    "def get_lati(value):\n",
    "    return get_coordinate(value)[1]\n",
    "\n",
    "# Register the UDF\n",
    "geocode_long_udf = udf(get_long, DoubleType())\n",
    "geocode_lati_udf = udf(get_lati, DoubleType())\n",
    "# geocode_udf = udf(get_coordinate, StructType([StructField(\"longitude\", DoubleType()), StructField(\"latitude\", DoubleType())]))\n",
    "\n",
    "# Apply the lambda function and create a new column\n",
    "df_coordinate = df_coordinate.withColumn(\"location\", concat_ws(\",\", df_coordinate[\"NAME_0\"], df_coordinate[\"NAME_1\"], df_coordinate[\"NAME_2\"], df_coordinate[\"NAME_3\"]).cast(StringType()))\n",
    "df_coordinate = df_coordinate.withColumn(\"longitude\", geocode_long_udf(df_coordinate[\"location\"]))\n",
    "df_coordinate = df_coordinate.withColumn(\"latitude\", geocode_lati_udf(df_coordinate[\"location\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0a07c7e-1eac-46ab-9430-9051d1e65605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GID_0: string (nullable = true)\n",
      " |-- NAME_0: string (nullable = true)\n",
      " |-- GID_1: string (nullable = true)\n",
      " |-- NAME_1: string (nullable = true)\n",
      " |-- GID_2: string (nullable = true)\n",
      " |-- NAME_2: string (nullable = true)\n",
      " |-- GID_3: string (nullable = true)\n",
      " |-- NAME_3: string (nullable = true)\n",
      " |-- GID_4: string (nullable = true)\n",
      " |-- NAME_4: string (nullable = true)\n",
      " |-- VARNAME_4: string (nullable = true)\n",
      " |-- TYPE_4: string (nullable = true)\n",
      " |-- ENGTYPE_4: string (nullable = true)\n",
      " |-- CC_4: string (nullable = true)\n",
      " |-- location: string (nullable = false)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_coordinate.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4db775d7-aae8-44cc-b04e-78326fa3c810",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 814, in main\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 650, in read_udfs\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 375, in read_single_udf\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 87, in read_command\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'geopy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write dataframe into csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf_coordinate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNAME_4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Indonesia_Coordinate_csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# df_coordinate.select(\"NAME_4\", \"location\").write.format(\"csv\").mode('overwrite').save(\"./Indonesia_Coordinate_csv\")\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 814, in main\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 650, in read_udfs\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 375, in read_single_udf\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 87, in read_command\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'geopy'\n"
     ]
    }
   ],
   "source": [
    "# Write dataframe into csv\n",
    "df_coordinate.select(\"NAME_4\", \"location\", \"longitude\", \"latitude\").write.format(\"csv\").mode('overwrite').save(\"./Indonesia_Coordinate_csv\")\n",
    "# df_coordinate.select(\"NAME_4\", \"location\").write.format(\"csv\").mode('overwrite').save(\"./Indonesia_Coordinate_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f539630-9444-42d6-ab1c-1afafd96175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|      NAME_4|            location|\n",
      "+------------+--------------------+\n",
      "|    Air Baru|Indonesia,Sumater...|\n",
      "|  Bumi Genap|Indonesia,Sumater...|\n",
      "|Gedung Nyawa|Indonesia,Sumater...|\n",
      "| Gedung Wani|Indonesia,Sumater...|\n",
      "|Karang Endah|Indonesia,Sumater...|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./Indonesia_Coordinate_csv\", StructType([StructField(\"NAME_4\", StringType(), True), StructField(\"location\", StringType(), True)]))\n",
    "print(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150439b-92d7-47f9-902e-47a17d976828",
   "metadata": {},
   "source": [
    "## Load Dataset Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03661f8c-2b7c-40a7-b14e-d355cf633819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee.Image({\n",
      "  \"functionInvocationValue\": {\n",
      "    \"functionName\": \"Collection.first\",\n",
      "    \"arguments\": {\n",
      "      \"collection\": {\n",
      "        \"functionInvocationValue\": {\n",
      "          \"functionName\": \"ImageCollection.load\",\n",
      "          \"arguments\": {\n",
      "            \"id\": {\n",
      "              \"constantValue\": \"ESA/WorldCover/v200\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "})\n",
      "{ 'bands': [ { 'crs': 'EPSG:4326',\n",
      "               'crs_transform': [ 8.333333333333333e-05,\n",
      "                                  0,\n",
      "                                  -180,\n",
      "                                  0,\n",
      "                                  -8.333333333333333e-05,\n",
      "                                  84],\n",
      "               'data_type': { 'max': 255,\n",
      "                              'min': 0,\n",
      "                              'precision': 'int',\n",
      "                              'type': 'PixelType'},\n",
      "               'dimensions': [ 4320000,\n",
      "                               1728000],\n",
      "               'id': 'Map'}],\n",
      "  'id': 'ESA/WorldCover/v200/2021',\n",
      "  'properties': { 'Map_class_names': [ 'Tree '\n",
      "                                       'cover',\n",
      "                                       'Shrubland',\n",
      "                                       'Grassland',\n",
      "                                       'Cropland',\n",
      "                                       'Built-up',\n",
      "                                       'Bare '\n",
      "                                       '/ '\n",
      "                                       'sparse '\n",
      "                                       'vegetation',\n",
      "                                       'Snow '\n",
      "                                       'and '\n",
      "                                       'ice',\n",
      "                                       'Permanent '\n",
      "                                       'water '\n",
      "                                       'bodies',\n",
      "                                       'Herbaceous '\n",
      "                                       'wetland',\n",
      "                                       'Mangroves',\n",
      "                                       'Moss '\n",
      "                                       'and '\n",
      "                                       'lichen'],\n",
      "                  'Map_class_palette': [ '006400',\n",
      "                                         'ffbb22',\n",
      "                                         'ffff4c',\n",
      "                                         'f096ff',\n",
      "                                         'fa0000',\n",
      "                                         'b4b4b4',\n",
      "                                         'f0f0f0',\n",
      "                                         '0064c8',\n",
      "                                         '0096a0',\n",
      "                                         '00cf75',\n",
      "                                         'fae6a0'],\n",
      "                  'Map_class_values': [ 10,\n",
      "                                        20,\n",
      "                                        30,\n",
      "                                        40,\n",
      "                                        50,\n",
      "                                        60,\n",
      "                                        70,\n",
      "                                        80,\n",
      "                                        90,\n",
      "                                        95,\n",
      "                                        100],\n",
      "                  'system:asset_size': 109661138990,\n",
      "                  'system:footprint': { 'coordinates': [ [ -180,\n",
      "                                                           -90],\n",
      "                                                         [ 180,\n",
      "                                                           -90],\n",
      "                                                         [ 180,\n",
      "                                                           90],\n",
      "                                                         [ -180,\n",
      "                                                           90],\n",
      "                                                         [ -180,\n",
      "                                                           -90]],\n",
      "                                        'type': 'LinearRing'},\n",
      "                  'system:index': '2021',\n",
      "                  'system:time_end': 1640991600000,\n",
      "                  'system:time_start': 1609455600000},\n",
      "  'type': 'Image',\n",
      "  'version': 1685064899986242}\n",
      "Number of bands: 1\n",
      "Projection: {'type': 'Projection', 'crs': 'EPSG:4326', 'transform': [8.333333333333333e-05, 0, -180, 0, -8.333333333333333e-05, 84]}\n",
      "Scale: 9.276624232772797\n"
     ]
    }
   ],
   "source": [
    "dataset = ee.ImageCollection('ESA/WorldCover/v200').first()\n",
    "print(dataset)\n",
    "\n",
    "# Print image object WITH call to getInfo(); prints image metadata.\n",
    "pp = pprint.PrettyPrinter(indent=2 ,width=15, compact=True)\n",
    "pp.pprint(dataset.getInfo())\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(\"Number of bands:\", dataset.bandNames().length().getInfo())\n",
    "print(\"Projection:\", dataset.projection().getInfo())\n",
    "print(\"Scale:\", dataset.projection().nominalScale().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a7debe-f569-4bad-95a2-ac494d91e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   location  latitude  longitude\n",
      "0  Surabaya    -7.124    112.545\n",
      "1      Bali    -8.397    114.951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b35ccf3ad544eea6f353f930c21b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[0.467, 114.193], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(child…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the visualization parameters\n",
    "visualization = {\n",
    "    'bands': ['Map']\n",
    "}\n",
    "\n",
    "# Center the map on the specified coordinates and zoom level\n",
    "center = [0.467, 114.193]\n",
    "zoom = 5\n",
    "\n",
    "# Display the dataset on the map using geemap\n",
    "Map = geemap.Map(center=center, zoom=zoom)\n",
    "Map.addLayer(dataset, visualization, 'Landcover')\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "Map.add_circle_markers_from_xy(data, x=\"longitude\", y=\"latitude\", radius=10, color=\"red\")\n",
    "\n",
    "print(data.head(5))\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebcd0c2-5c59-4c15-909c-455d98842ecd",
   "metadata": {},
   "source": [
    "## Load Dataset Indonesia ShapeFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57aebb60-475b-4aa8-9f7a-3378dfdaa3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 10) (502, 13) (6696, 16) (77474, 14)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'D:\\\\00 Project\\\\00 My Project\\\\Dataset\\\\Revalue Nature\\\\Case 1\\\\gadm36_IDN_shp\\\\'\n",
    "file_1 = 'gadm36_IDN_1.dbf'\n",
    "file_2 = 'gadm36_IDN_2.dbf'\n",
    "file_3 = 'gadm36_IDN_3.dbf'\n",
    "file_4 = 'gadm36_IDN_4.dbf'\n",
    "dbf1 = Dbf5(file_path+file_1)\n",
    "df1 = dbf1.to_dataframe()\n",
    "dbf2 = Dbf5(file_path+file_2)\n",
    "df2 = dbf2.to_dataframe()\n",
    "dbf3 = Dbf5(file_path+file_3)\n",
    "df3 = dbf3.to_dataframe()\n",
    "dbf4 = Dbf5(file_path+file_4)\n",
    "df4 = dbf4.to_dataframe()\n",
    "print(df1.shape, df2.shape, df3.shape, df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24efdd8b-b9b2-4b2d-8d69-4d2466ee8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GID_0         object\n",
      "NAME_0        object\n",
      "GID_1         object\n",
      "NAME_1        object\n",
      "GID_2         object\n",
      "NAME_2        object\n",
      "GID_3         object\n",
      "NAME_3        object\n",
      "GID_4         object\n",
      "NAME_4        object\n",
      "VARNAME_4    float64\n",
      "TYPE_4        object\n",
      "ENGTYPE_4     object\n",
      "CC_4          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a1d0356-9023-412e-a11c-26c7eab5c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      GID_0     NAME_0    GID_1 NAME_1      GID_2        NAME_2        GID_3  \\\n",
      "12529   IDN  Indonesia  IDN.8_1  Jambi  IDN.8.8_1  Sungai Penuh  IDN.8.8.1_1   \n",
      "\n",
      "      NAME_3          GID_4       NAME_4 VARNAME_4     TYPE_4  \\\n",
      "12529         IDN.8.8.1.1_1  Sumur Anyir            Kelurahan   \n",
      "\n",
      "             ENGTYPE_4        CC_4  \n",
      "12529  Urban Community  1572030009  \n",
      "      GID_0     NAME_0     GID_1            NAME_1        GID_2    NAME_2  \\\n",
      "60522   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "60523   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "60524   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "60525   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "\n",
      "               GID_3    NAME_3            GID_4            NAME_4 VARNAME_4  \\\n",
      "60522  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.1_1                               \n",
      "60523  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.2_1    Galung Maloang             \n",
      "60524  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.3_1             Lemoe             \n",
      "60525  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.4_1  Wattang Bacukiki             \n",
      "\n",
      "          TYPE_4        ENGTYPE_4        CC_4  \n",
      "60522  Kelurahan  Urban Community  7372010004  \n",
      "60523  Kelurahan  Urban Community  7372010005  \n",
      "60524  Kelurahan  Urban Community  7372010003  \n",
      "60525  Kelurahan  Urban Community  7372010002  \n"
     ]
    }
   ],
   "source": [
    "# Data cleansing\n",
    "df4 = df4.replace(np.nan, '', regex=True)\n",
    "print(df4[(df4['NAME_2'] == 'Sungai Penuh') & (df4['NAME_4'] == 'Sumur Anyir')])\n",
    "print(df4[(df4['NAME_2'] == 'Parepare') & (df4['NAME_3'] == 'Bacukiki')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f706d-0c5c-4231-8051-f0bbbd9ee85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all locations column into single column\n",
    "df4['location'] = df4[[\"NAME_0\", 'NAME_1', 'NAME_2', 'NAME_3']].apply(\",\".join, axis=1)\n",
    "# # df4['location'] = df4[['NAME_0', 'NAME_1', 'NAME_2', 'NAME_3']].agg(', '.join, axis=1)\n",
    "\n",
    "# For Testing purposes\n",
    "# df5 = df4[(df4['NAME_2'] == 'Sungai Penuh') & (df4['NAME_4'] == 'Sumur Anyir') | ((df4['NAME_2'] == 'Parepare') & (df4['NAME_3'] == 'Bacukiki'))]\n",
    "# df5 = df4[df4['NAME_2'] == 'Parepare']\n",
    "df5 = df4\n",
    "\n",
    "df5['coordinate'] = df5['location'].apply(lambda x: get_coordinate(x))\n",
    "df5['longitude'] = df5['coordinate'].apply(lambda x: x[0])\n",
    "df5['latitude'] = df5['coordinate'].apply(lambda x: x[1])\n",
    "print(df5.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936444dc-91b3-47d6-a60e-d5c449ad3604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf19a2-3dee-4251-981e-2d1f22cce785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
