{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8c498d-0599-4a6e-a311-fbaaf61227b3",
   "metadata": {},
   "source": [
    "# Use Case 1 - Calculate Area of Land Cove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d17a0-cebc-4e6b-8d25-d25e28c587d0",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48019c43-f3a9-4c06-9c3d-6e1ef8d8b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=Pzp1IgF5V_eb9pyFu_mzUPzgF6xQoEHyXVJbfVJoqeg&tc=O5CanZw4VymiZ57f8gr8JLqEmLsgPW-fuydPNZDgHg4&cc=NVTk-5J9Tzn2BTNXpXSVCsPBBL2C2DxhbysBxNmtrNg>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=Pzp1IgF5V_eb9pyFu_mzUPzgF6xQoEHyXVJbfVJoqeg&tc=O5CanZw4VymiZ57f8gr8JLqEmLsgPW-fuydPNZDgHg4&cc=NVTk-5J9Tzn2BTNXpXSVCsPBBL2C2DxhbysBxNmtrNg</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1Adeu5BVHIOio1Ydl4lpLN-t9YoU24plmIgtaGEANjLjSa26vV0IgOlXrQKg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Google Earth Initialization\n",
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd7e936-c768-4645-86e1-eb84b5c87287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib Initialization\n",
    "from simpledbf import Dbf5\n",
    "from geopy.geocoders import Nominatim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geemap\n",
    "import pprint\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from dbfread import DBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1edfe-cfbb-43a3-9ab0-572cfc57e6e2",
   "metadata": {},
   "source": [
    "## Load Indonesia Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25640476-b11e-4676-a1f0-61d52ffed187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e498b62d-94bb-45dc-89de-11a5fcfed124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Spark Session\n",
    "sparkMaster = 'local[*]'\n",
    "sparkAppName = 'Py-RevalueNature-Case2'\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(sparkMaster) \\\n",
    "    .appName(sparkAppName) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c67961d6-58f2-459f-8ee7-f460a9db4cb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o146.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 17) (192.168.1.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.11 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.11 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# create a PySpark DataFrame from the data\u001b[39;00m\n\u001b[0;32m     29\u001b[0m df_indonesia \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, schema)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mdf_indonesia\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o146.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 17) (192.168.1.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.11 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\03 Data Tools\\spark-3.4.1-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.11 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Load Indonesia Dataset\n",
    "file_path = 'D:\\\\00 Project\\\\00 My Project\\\\Dataset\\\\Revalue Nature\\\\Case 1\\\\gadm36_IDN_shp\\\\'\n",
    "file_4 = 'gadm36_IDN_4.dbf'\n",
    "records = DBF(file_path+file_4, encoding=\"latin1\")\n",
    "\n",
    "# define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"GID_0\", StringType(), True),\n",
    "    StructField(\"NAME_0\", StringType(), True),\n",
    "    StructField(\"GID_1\", StringType(), True),\n",
    "    StructField(\"NAME_1\", StringType(), True),\n",
    "    StructField(\"GID_2\", StringType(), True),\n",
    "    StructField(\"NAME_2\", StringType(), True),\n",
    "    StructField(\"GID_3\", StringType(), True),\n",
    "    StructField(\"NAME_3\", StringType(), True),\n",
    "    StructField(\"GID_4\", StringType(), True),\n",
    "    StructField(\"NAME_4\", StringType(), True),\n",
    "    StructField(\"VARNAME_4\", StringType(), True),\n",
    "    StructField(\"TYPE_4\", StringType(), True),\n",
    "    StructField(\"ENGTYPE_4\", StringType(), True),\n",
    "    StructField(\"CC_4\", StringType(), True),\n",
    "    # add more fields as needed\n",
    "])\n",
    "\n",
    "# create a list of dictionaries containing the data\n",
    "data = [dict(record) for record in records]\n",
    "\n",
    "# create a PySpark DataFrame from the data\n",
    "df_indonesia = spark.createDataFrame(data, schema)\n",
    "df_indonesia.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b91a959-f5b3-43ec-8804-474312eb4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+----+---------+------+-----------+--------+-------+--------+--------+--------+--------+\n",
      "|   _c0|    _c1|  _c2| _c3|      _c4|   _c5|        _c6|     _c7|    _c8|     _c9|    _c10|    _c11|    _c12|\n",
      "+------+-------+-----+----+---------+------+-----------+--------+-------+--------+--------+--------+--------+\n",
      "|Number|Digimon|Stage|Type|Attribute|Memory|Equip Slots|Lv 50 HP|Lv50 SP|Lv50 Atk|Lv50 Def|Lv50 Int|Lv50 Spd|\n",
      "|     1|Kuramon| Baby|Free|  Neutral|     2|          0|     590|     77|      79|      69|      68|      95|\n",
      "|     2|Pabumon| Baby|Free|  Neutral|     2|          0|     950|     62|      76|      76|      69|      68|\n",
      "|     3|Punimon| Baby|Free|  Neutral|     2|          0|     870|     50|      97|      87|      50|      75|\n",
      "|     4|Botamon| Baby|Free|  Neutral|     2|          0|     690|     68|      77|      95|      76|      61|\n",
      "+------+-------+-----+----+---------+------+-----------+--------+-------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Load Digimon Dataset\n",
    "digimon_folderpath = 'D:\\\\00 Project\\\\00 My Project\\\\Dataset\\\\Digimon Dataset\\\\'\n",
    "digimon_filename = 'DigiDB_digimonlist.csv'\n",
    "df_digimon = spark.read.format(\"csv\").load(digimon_folderpath+digimon_filename)\n",
    "df_digimon.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a67fbcd-c053-4277-9068-8eb2d027b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longitude of the location is:  113.54024450668652\n",
      "The latitude of the location is:  -8.28236725\n"
     ]
    }
   ],
   "source": [
    "# Initialize Nominatim API\n",
    "geolocator = Nominatim(user_agent=\"Geo Locator\")\n",
    "\n",
    "def get_coordinate(location):\n",
    "    try:\n",
    "        coordinate = geolocator.geocode(location)\n",
    "        return (coordinate.longitude, coordinate.latitude)\n",
    "    except:\n",
    "        return (0, 0)\n",
    "\n",
    "coordinate = get_coordinate(\"Indonesia, Jawa Timur, Jember, Balung\")\n",
    "print(\"The longitude of the location is: \", coordinate[0])\n",
    "print(\"The latitude of the location is: \", coordinate[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150439b-92d7-47f9-902e-47a17d976828",
   "metadata": {},
   "source": [
    "## Load Dataset Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03661f8c-2b7c-40a7-b14e-d355cf633819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee.Image({\n",
      "  \"functionInvocationValue\": {\n",
      "    \"functionName\": \"Collection.first\",\n",
      "    \"arguments\": {\n",
      "      \"collection\": {\n",
      "        \"functionInvocationValue\": {\n",
      "          \"functionName\": \"ImageCollection.load\",\n",
      "          \"arguments\": {\n",
      "            \"id\": {\n",
      "              \"constantValue\": \"ESA/WorldCover/v200\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "})\n",
      "{ 'bands': [ { 'crs': 'EPSG:4326',\n",
      "               'crs_transform': [ 8.333333333333333e-05,\n",
      "                                  0,\n",
      "                                  -180,\n",
      "                                  0,\n",
      "                                  -8.333333333333333e-05,\n",
      "                                  84],\n",
      "               'data_type': { 'max': 255,\n",
      "                              'min': 0,\n",
      "                              'precision': 'int',\n",
      "                              'type': 'PixelType'},\n",
      "               'dimensions': [ 4320000,\n",
      "                               1728000],\n",
      "               'id': 'Map'}],\n",
      "  'id': 'ESA/WorldCover/v200/2021',\n",
      "  'properties': { 'Map_class_names': [ 'Tree '\n",
      "                                       'cover',\n",
      "                                       'Shrubland',\n",
      "                                       'Grassland',\n",
      "                                       'Cropland',\n",
      "                                       'Built-up',\n",
      "                                       'Bare '\n",
      "                                       '/ '\n",
      "                                       'sparse '\n",
      "                                       'vegetation',\n",
      "                                       'Snow '\n",
      "                                       'and '\n",
      "                                       'ice',\n",
      "                                       'Permanent '\n",
      "                                       'water '\n",
      "                                       'bodies',\n",
      "                                       'Herbaceous '\n",
      "                                       'wetland',\n",
      "                                       'Mangroves',\n",
      "                                       'Moss '\n",
      "                                       'and '\n",
      "                                       'lichen'],\n",
      "                  'Map_class_palette': [ '006400',\n",
      "                                         'ffbb22',\n",
      "                                         'ffff4c',\n",
      "                                         'f096ff',\n",
      "                                         'fa0000',\n",
      "                                         'b4b4b4',\n",
      "                                         'f0f0f0',\n",
      "                                         '0064c8',\n",
      "                                         '0096a0',\n",
      "                                         '00cf75',\n",
      "                                         'fae6a0'],\n",
      "                  'Map_class_values': [ 10,\n",
      "                                        20,\n",
      "                                        30,\n",
      "                                        40,\n",
      "                                        50,\n",
      "                                        60,\n",
      "                                        70,\n",
      "                                        80,\n",
      "                                        90,\n",
      "                                        95,\n",
      "                                        100],\n",
      "                  'system:asset_size': 109661138990,\n",
      "                  'system:footprint': { 'coordinates': [ [ -180,\n",
      "                                                           -90],\n",
      "                                                         [ 180,\n",
      "                                                           -90],\n",
      "                                                         [ 180,\n",
      "                                                           90],\n",
      "                                                         [ -180,\n",
      "                                                           90],\n",
      "                                                         [ -180,\n",
      "                                                           -90]],\n",
      "                                        'type': 'LinearRing'},\n",
      "                  'system:index': '2021',\n",
      "                  'system:time_end': 1640991600000,\n",
      "                  'system:time_start': 1609455600000},\n",
      "  'type': 'Image',\n",
      "  'version': 1685064899986242}\n",
      "Number of bands: 1\n",
      "Projection: {'type': 'Projection', 'crs': 'EPSG:4326', 'transform': [8.333333333333333e-05, 0, -180, 0, -8.333333333333333e-05, 84]}\n",
      "Scale: 9.276624232772797\n"
     ]
    }
   ],
   "source": [
    "dataset = ee.ImageCollection('ESA/WorldCover/v200').first()\n",
    "print(dataset)\n",
    "\n",
    "# Print image object WITH call to getInfo(); prints image metadata.\n",
    "pp = pprint.PrettyPrinter(indent=2 ,width=15, compact=True)\n",
    "pp.pprint(dataset.getInfo())\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(\"Number of bands:\", dataset.bandNames().length().getInfo())\n",
    "print(\"Projection:\", dataset.projection().getInfo())\n",
    "print(\"Scale:\", dataset.projection().nominalScale().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a7debe-f569-4bad-95a2-ac494d91e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   location  latitude  longitude\n",
      "0  Surabaya    -7.124    112.545\n",
      "1      Bali    -8.397    114.951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b35ccf3ad544eea6f353f930c21b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[0.467, 114.193], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(childâ€¦"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the visualization parameters\n",
    "visualization = {\n",
    "    'bands': ['Map']\n",
    "}\n",
    "\n",
    "# Center the map on the specified coordinates and zoom level\n",
    "center = [0.467, 114.193]\n",
    "zoom = 5\n",
    "\n",
    "# Display the dataset on the map using geemap\n",
    "Map = geemap.Map(center=center, zoom=zoom)\n",
    "Map.addLayer(dataset, visualization, 'Landcover')\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "Map.add_circle_markers_from_xy(data, x=\"longitude\", y=\"latitude\", radius=10, color=\"red\")\n",
    "\n",
    "print(data.head(5))\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebcd0c2-5c59-4c15-909c-455d98842ecd",
   "metadata": {},
   "source": [
    "## Load Dataset Indonesia ShapeFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57aebb60-475b-4aa8-9f7a-3378dfdaa3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 10) (502, 13) (6696, 16) (77474, 14)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'D:\\\\00 Project\\\\00 My Project\\\\Dataset\\\\Revalue Nature\\\\Case 1\\\\gadm36_IDN_shp\\\\'\n",
    "file_1 = 'gadm36_IDN_1.dbf'\n",
    "file_2 = 'gadm36_IDN_2.dbf'\n",
    "file_3 = 'gadm36_IDN_3.dbf'\n",
    "file_4 = 'gadm36_IDN_4.dbf'\n",
    "dbf1 = Dbf5(file_path+file_1)\n",
    "df1 = dbf1.to_dataframe()\n",
    "dbf2 = Dbf5(file_path+file_2)\n",
    "df2 = dbf2.to_dataframe()\n",
    "dbf3 = Dbf5(file_path+file_3)\n",
    "df3 = dbf3.to_dataframe()\n",
    "dbf4 = Dbf5(file_path+file_4)\n",
    "df4 = dbf4.to_dataframe()\n",
    "print(df1.shape, df2.shape, df3.shape, df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24efdd8b-b9b2-4b2d-8d69-4d2466ee8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GID_0         object\n",
      "NAME_0        object\n",
      "GID_1         object\n",
      "NAME_1        object\n",
      "GID_2         object\n",
      "NAME_2        object\n",
      "GID_3         object\n",
      "NAME_3        object\n",
      "GID_4         object\n",
      "NAME_4        object\n",
      "VARNAME_4    float64\n",
      "TYPE_4        object\n",
      "ENGTYPE_4     object\n",
      "CC_4          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a1d0356-9023-412e-a11c-26c7eab5c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      GID_0     NAME_0    GID_1 NAME_1      GID_2        NAME_2        GID_3  \\\n",
      "12529   IDN  Indonesia  IDN.8_1  Jambi  IDN.8.8_1  Sungai Penuh  IDN.8.8.1_1   \n",
      "\n",
      "      NAME_3          GID_4       NAME_4 VARNAME_4     TYPE_4  \\\n",
      "12529         IDN.8.8.1.1_1  Sumur Anyir            Kelurahan   \n",
      "\n",
      "             ENGTYPE_4        CC_4  \n",
      "12529  Urban Community  1572030009  \n",
      "      GID_0     NAME_0     GID_1            NAME_1        GID_2    NAME_2  \\\n",
      "60522   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "60523   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "60524   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "60525   IDN  Indonesia  IDN.26_1  Sulawesi Selatan  IDN.26.16_1  Parepare   \n",
      "\n",
      "               GID_3    NAME_3            GID_4            NAME_4 VARNAME_4  \\\n",
      "60522  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.1_1                               \n",
      "60523  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.2_1    Galung Maloang             \n",
      "60524  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.3_1             Lemoe             \n",
      "60525  IDN.26.16.1_1  Bacukiki  IDN.26.16.1.4_1  Wattang Bacukiki             \n",
      "\n",
      "          TYPE_4        ENGTYPE_4        CC_4  \n",
      "60522  Kelurahan  Urban Community  7372010004  \n",
      "60523  Kelurahan  Urban Community  7372010005  \n",
      "60524  Kelurahan  Urban Community  7372010003  \n",
      "60525  Kelurahan  Urban Community  7372010002  \n"
     ]
    }
   ],
   "source": [
    "# Data cleansing\n",
    "df4 = df4.replace(np.nan, '', regex=True)\n",
    "print(df4[(df4['NAME_2'] == 'Sungai Penuh') & (df4['NAME_4'] == 'Sumur Anyir')])\n",
    "print(df4[(df4['NAME_2'] == 'Parepare') & (df4['NAME_3'] == 'Bacukiki')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f706d-0c5c-4231-8051-f0bbbd9ee85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all locations column into single column\n",
    "df4['location'] = df4[[\"NAME_0\", 'NAME_1', 'NAME_2', 'NAME_3']].apply(\",\".join, axis=1)\n",
    "# # df4['location'] = df4[['NAME_0', 'NAME_1', 'NAME_2', 'NAME_3']].agg(', '.join, axis=1)\n",
    "\n",
    "# For Testing purposes\n",
    "# df5 = df4[(df4['NAME_2'] == 'Sungai Penuh') & (df4['NAME_4'] == 'Sumur Anyir') | ((df4['NAME_2'] == 'Parepare') & (df4['NAME_3'] == 'Bacukiki'))]\n",
    "# df5 = df4[df4['NAME_2'] == 'Parepare']\n",
    "df5 = df4\n",
    "\n",
    "df5['coordinate'] = df5['location'].apply(lambda x: get_coordinate(x))\n",
    "df5['longitude'] = df5['coordinate'].apply(lambda x: x[0])\n",
    "df5['latitude'] = df5['coordinate'].apply(lambda x: x[1])\n",
    "print(df5.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936444dc-91b3-47d6-a60e-d5c449ad3604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf19a2-3dee-4251-981e-2d1f22cce785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
